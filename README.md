# RNN进行文本生成

## 题目简介

自然语言处理（NLP）的文本生成是一门复杂而强大的技术领域，通过深度学习和自然语言处理算法的结合，模型能够理解并生成出与人类语言相近甚至更加自然流畅的文本。这项技术涉及多层次的学习，从单词和语法结构到语义和上下文的综合考量，使得模型能够以多样的方式生成出准确、连贯且富有逼真感的文本内容。文本生成的应用领域极为广泛，包括但不限于自动文案创作、新闻报道、故事和诗歌创作、对话系统、自动摘要生成以及智能客服对话等。这项技术的发展不仅推动了人机交互的智能化进程，还为各行各业提供了强大的语言处理和沟通工具，提高了工作效率、创造了新的应用场景，并推动了智能化系统和服务的不断发展和进步。

在本题目中，我们将使用最全中文诗歌古典文集数据库进行学习，并且使用RNN进行诗歌的生成。



## 项目要求

1.	熟悉各种网络的框架，能熟练使用Pytorch搭建神经网络，了解每一个模块的作用；
1.	熟悉自然语言处理领域中对数据的预处理方法；
1.	要求使用多种网络（RNN、CNN或是它们的变体）实验，记录网络表现情况，并且进行对比；


## 数据集

主页
https://github.com/chinese-poetry/chinese-poetry

本项目使用部分
https://github.com/chinese-poetry/chinese-poetry/tree/master/%E5%85%A8%E5%94%90%E8%AF%97

## 过程

### 数据预处理

在dealdata.py文件中

定义deal_data函数：将.json文件的诗歌数据录入到.txt文件中

定义dest函数：后续在main中引入此函数继续训练

### 定义rnn_lstm模型

在rnn_lstm.py文件中

定义weights_init函数：用于初始化神经网络中的权重

定义word_embedding：一个词嵌入模型，它将词的索引转换为词嵌入向量。

定义RNN_model：一个基于 LSTM 的循环神经网络模型，它用于生成诗歌。

RNN 的基本原理是利用神经网络的输出作为下一步的输入，形成一种“循环”的结构，这使得 RNN 能够处理序列数据。然而，传统的 RNN 存在梯度消失和梯度爆炸的问题，这使得它难以处理长序列数据。

LSTM 通过引入“门”结构和“记忆细胞”来解决这个问题。门结构可以控制信息的流动，记忆细胞可以存储长期的状态信息。这使得 LSTM 能够更好地处理长序列数据。

### 训练

在main.py文件中

定义process_poems函数：处理数据，映射得到词汇表

初始化词嵌入层和RNN模型

定义优化器      RMSprop

定义损失函数    NLLLoss(负对数似然损失)

定义generate_batch函数：生成批次数据并处理，计算损失进行反向传播

保存模型

## 效果展示

选出几个较好的效果

    (仅输入为Enter退出)请给出一个字:新
    新春光不可尋
    君不有时意，今日不相思。
    可怜君不见，空有一生情。

    (仅输入为Enter退出)请给出一个字:望
    望見千里春
    何时不可见，今日不可寻。
    一身不可见，今日不可为。

    (仅输入为Enter退出)请给出一个字:照
    照雲正秋
    风流落日照，风吹照水空。
    风光飞不可，归去不知知。

    (仅输入为Enter退出)请给出一个字:霜
    霜露滴滴花香
    风流落日照云色，风吹花枝落露香。
    谁家一朝有文子，不知君不可如头。



## 实验环境

### Windows

10.0.22621

### Python 

3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]

### Torch

2.2.1+cu121

Device 0: NVIDIA GeForce RTX 4060 Laptop GPU

Total memory: 8.0GB

Compute capability: 8.9


## 参考文献


基于项目 https://github.com/nndl/exercise/blob/master/chap6_RNN

改进数据预处理部分

调整训练参数进行优化


