
# 关于优化


##  **Dropout**

尝试调整dropout比例值以改善模型的性能。

ReLU激活函数之后的Dropout层和全连接层之前的Dropout层都是为了防止过拟合

1. ReLU激活函数之后的Dropout层：ReLU激活函数的输出是非负的，因此在ReLU之后添加Dropout层可以随机将一些神经元的输出设置为0，这样可以增加模型的稳健性，使模型对输入的小变化更加鲁棒。同时，这也可以防止模型过度依赖某些特定的神经元，从而防止过拟合。

2. 全连接层之前的Dropout层：全连接层的作用是将前一层的输出映射到新的空间，如果模型过度依赖某些特定的神经元，可能会导致过拟合。在全连接层之前添加Dropout层可以随机将一些神经元的输出设置为0，这样可以防止模型过度依赖某些特定的神经元，从而防止过拟合。





## **优化器**

尝试不同的优化器，如Adam、RMSprop等以改善模型的性能。


1. **随机梯度下降（SGD）**：这是最基本的优化器，它的主要思想是沿着梯度的反方向更新参数以最小化损失函数。SGD的一个主要问题是它可能会陷入局部最优解，而不是全局最优解。

2. **带动量的SGD**：动量可以帮助SGD在相关方向上加速，减少振荡，从而更快地收敛。

3. **Adagrad**：Adagrad会为每个参数保持一个学习率，这使得它能够对稀疏数据进行有效的优化。但是，Adagrad的学习率在训练过程中是单调递减的，这可能会导致训练过早停止。

4. **RMSprop**：RMSprop通过使用一个衰减系数来解决Adagrad学习率过早下降的问题，使得它在非凸优化问题上表现得更好。

5. **Adam**：Adam结合了RMSprop和动量的思想，它既有自适应学习率的特性，又有动量项。Adam通常被认为是一个在许多任务上表现都很好的优化器。


    - Adam一般作为首选，它在许多任务上都表现得相当好。
    - 如果数据稀疏或处理非凸优化问题，RMSprop或Adagrad会更好。
    - 如果损失函数波动较大，SGD更好。




## **激活函数**

尝试使用其他的激活函数以改善模型的性能。


1. **ReLU（Rectified Linear Unit）**：ReLU是最常用的激活函数，特别是在卷积神经网络（CNN）和深度学习模型中。ReLU函数在输入大于0时直接输出该值，在输入小于0时输出0。ReLU函数的优点是计算简单，而且不会出现梯度消失问题。但是，ReLU函数也有所谓的"死亡ReLU"问题，即某些神经元可能永远不会被激活，导致相应的参数无法更新。

2. **Sigmoid**：Sigmoid函数可以将任何输入转换为0到1之间的输出，因此常用于二分类问题的最后一层，表示概率输出。但是，Sigmoid函数在输入值的绝对值较大时，梯度接近0，容易出现梯度消失问题。

3. **Tanh**：Tanh函数的输出范围是-1到1，因此比Sigmoid函数的输出范围更广。Tanh函数在隐藏层中的使用比Sigmoid函数更常见。但是，Tanh函数仍然存在梯度消失问题。

4. **Leaky ReLU**：Leaky ReLU是ReLU的一个变种，解决了"死亡ReLU"问题。Leaky ReLU允许在输入小于0时有一个小的正斜率，而不是完全输出0。

5. **Softmax**：Softmax函数常用于多分类问题的最后一层，可以将一组输入转换为概率分布。



## **网络结构**

尝试使用LSTM或者其他的RNN结构以改善模型的性能。

尝试增加隐藏层的数量或使用更复杂的RNN结构，双向RNN或者堆叠RNN。


1. **双向RNN**：双向RNN可以同时处理过去和未来的信息。在某些任务中，如语言模型或者序列标注，双向RNN可以取得更好的效果。

2. **堆叠RNN**：堆叠RNN是指在一个RNN的输出上再叠加一个RNN。这可以使模型有更深的层次，能够学习到更复杂的模式。

3. **注意力机制**：注意力机制可以让模型在生成输出时，对输入的不同部分赋予不同的注意力。这在一些任务中，如机器翻译或者文本摘要，可以取得更好的效果。

4. **Transformer**：Transformer是一种基于自注意力机制的模型，它在许多NLP任务中都取得了最好的效果。






## **批量归一化**


尝试在全连接层之后添加Batch Normalization(批量归一化)以改善模型的性能。


Batch Normalization的主要思想是对每一层的输入进行归一化处理，使得结果的均值为0，方差为1。这样做的好处是可以防止梯度消失或梯度爆炸问题，使得网络可以使用更高的学习率，从而加速训练过程。

Batch Normalization的操作通常在全连接层或卷积层之后、激活函数之前进行。具体操作如下：

1. 计算当前批次数据的均值和方差。
2. 使用均值和方差对当前批次数据进行归一化处理。
3. 对归一化的结果进行缩放和平移，这两个操作的参数是可以学习的。



